# -*- coding: utf-8 -*-
"""BankChurnModeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IWQ--1vN4f-26_3cSXqMflYAK5hrHRWU

# **bank customer churn prediction**

Take this dataset for bank customer churn prediction : https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

data= pd.read_csv('Churn_Modelling.csv')

print(len(data))
data.head(2)

"""**visualize the data to get understanding**"""

# checking the gender that how many females and males that are leaving the bank
# 1 --> Yes, 0 --> No
gender_exit_1 = data[data.Exited == 1].Gender
print(len(gender_exit_1))
gender_exit_0 = data[data.Exited == 0].Gender
print(len(gender_exit_0))

# creating the function to create the histogram
def plot_hist(x, y, *k, t):
    # lets plot the data using the matplot histogram
    plt.hist([x, y, *k], color=['red', 'green'], label=['yes', 'no'])
    plt.title(t)
    plt.ylabel('No.of customers')
    plt.legend()
    plt.show()

plot_hist(gender_exit_1, gender_exit_0, t='Gender wise Exited')

# extracting the customers that exit or not based on geography
geography_exit_1 = data[data.Exited == 1].Geography
geography_exit_0 = data[data.Exited == 0].Geography

data.Geography.value_counts() # checking the value count

plot_hist(geography_exit_1, geography_exit_0, t='Based on location')

# checking the active members
isActive_1 = data[data.IsActiveMember == 1].Geography 
isActive_0 = data[data.IsActiveMember == 0].Geography
print(len(isActive_0), len(isActive_1))

plot_hist(isActive_1, isActive_0, t='No of Active users based on location')

"""# **PreProcessing Part**"""

# drop the columns that are not needed to predictions
data.head(2)

# dropping RowNumber, CustomerId, Surname, --> These are no need anymore
data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)

data.head()

# extracting the X and Y
X = data.drop('Exited', axis=1)
Y = data.Exited

X.head(2)

Y.head(2)

X.dtypes

# here above all the datatypes are int and float 
# here we need to handle the categorical data
# i am using the pd.dummies to one hot encoding
categorical_geography=pd.get_dummies(data.Geography)
gender_categorical = pd.get_dummies(data.Gender)

# dropping the gender and geography
X.drop(['Gender', 'Geography'], axis=1, inplace=True)

X.head()

# concatenating the data together after doing one hot encodeing of gender and geography
X = pd.concat([X, categorical_geography, gender_categorical], axis=1)

X.isnull().sum() # checking the null values

# here Balance have some small numbers and large so normalizing the data using MinMaxScaler
# MinMaxScaler wil convert every number between 0 and 1
from sklearn.preprocessing import MinMaxScaler
scale = ['Balance', 'EstimatedSalary']
scaler = MinMaxScaler()
X[scale] = scaler.fit_transform(X[scale])

X

"""# Splitting the data into train and test"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y)

X_train.shape

type(X_train)

from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential

# create a ann model 
# layer1 takes the input 13 units are giving randomly 100
# layer2 consiste 10 units
# layer3 is the output layer 1
model = Sequential(
    [
        Dense(100, input_shape=(13, ), activation='relu'),
        Dense(10, activation='relu'),
        Dense(1, activation='sigmoid')
    ]
)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
early = EarlyStopping(patience=3)
model.fit(X_train, Y_train, epochs=20, validation_data=(X_test, Y_test), callbacks=[early])

print('evaluate', model.evaluate(X_test, Y_test))
Y_pred_array = model.predict(X_test)
Y_pred = []
for i in Y_pred_array:
  if i > 0.5:
    Y_pred.append(1)
  else:
    Y_pred.append(0)
df = pd.DataFrame({'Y_actual':Y_test, 'Y_pred': Y_pred})
df.Y_pred.value_counts()
Y_pred = np.array(Y_pred)
from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))

print(df.Y_pred.value_counts())
df.Y_actual.value_counts()

"""# **the results may be different because we are doing random splitting becaute the majority class is 0**

# if you see in the original data majority customers are not leaving is 0 --> Not_Existing
# if you see in the original data few customer are leaving is 1 --> Existing
"""

data.Exited.value_counts()